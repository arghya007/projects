{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42345303",
   "metadata": {
    "papermill": {
     "duration": 0.006468,
     "end_time": "2023-02-28T18:52:43.846759",
     "exception": false,
     "start_time": "2023-02-28T18:52:43.840291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c87d2c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-28T18:52:43.860274Z",
     "iopub.status.busy": "2023-02-28T18:52:43.859261Z",
     "iopub.status.idle": "2023-02-28T18:59:17.513125Z",
     "shell.execute_reply": "2023-02-28T18:59:17.510465Z"
    },
    "papermill": {
     "duration": 393.664005,
     "end_time": "2023-02-28T18:59:17.516640",
     "exception": false,
     "start_time": "2023-02-28T18:52:43.852635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-28 18:52:52--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 5828358084 (5.4G) [application/zip]\r\n",
      "Saving to: ‘crawl-300d-2M-subword.zip’\r\n",
      "\r\n",
      "crawl-300d-2M-subwo 100%[===================>]   5.43G  21.7MB/s    in 4m 27s  \r\n",
      "\r\n",
      "2023-02-28 18:57:20 (20.8 MB/s) - ‘crawl-300d-2M-subword.zip’ saved [5828358084/5828358084]\r\n",
      "\r\n",
      "Archive:  crawl-300d-2M-subword.zip\r\n",
      "  inflating: crawl-300d-2M-subword.vec  \r\n",
      "  inflating: crawl-300d-2M-subword.bin  \r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize.simple import *\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import groupby\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.utils import class_weight\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score, precision_score, matthews_corrcoef, roc_auc_score, balanced_accuracy_score, recall_score, f1_score, make_scorer\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
    "!unzip crawl-300d-2M-subword.zip\n",
    "pretrained = fasttext.FastText.load_model('crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a71d7",
   "metadata": {
    "papermill": {
     "duration": 0.063649,
     "end_time": "2023-02-28T18:59:17.646483",
     "exception": false,
     "start_time": "2023-02-28T18:59:17.582834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b896be3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T18:59:17.779486Z",
     "iopub.status.busy": "2023-02-28T18:59:17.777716Z",
     "iopub.status.idle": "2023-02-28T18:59:18.082425Z",
     "shell.execute_reply": "2023-02-28T18:59:18.080745Z"
    },
    "papermill": {
     "duration": 0.373932,
     "end_time": "2023-02-28T18:59:18.085169",
     "exception": false,
     "start_time": "2023-02-28T18:59:17.711237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/isarcasm-tweets-rephrase/iSarcasm.csv')\n",
    "news = pd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a38ef",
   "metadata": {
    "papermill": {
     "duration": 0.063027,
     "end_time": "2023-02-28T18:59:18.211212",
     "exception": false,
     "start_time": "2023-02-28T18:59:18.148185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Handeling Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd81371c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T18:59:18.340138Z",
     "iopub.status.busy": "2023-02-28T18:59:18.339455Z",
     "iopub.status.idle": "2023-02-28T18:59:18.404203Z",
     "shell.execute_reply": "2023-02-28T18:59:18.402750Z"
    },
    "papermill": {
     "duration": 0.132233,
     "end_time": "2023-02-28T18:59:18.407364",
     "exception": false,
     "start_time": "2023-02-28T18:59:18.275131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['tweet'])\n",
    "data[['sarcasm','irony','satire','understatement','overstatement','rhetorical_question']] = data[['sarcasm','irony','satire','understatement','overstatement','rhetorical_question']].fillna(0)\n",
    "data['rephrase'] = data['rephrase'].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3755e6f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T18:59:18.541878Z",
     "iopub.status.busy": "2023-02-28T18:59:18.541487Z",
     "iopub.status.idle": "2023-02-28T18:59:18.563965Z",
     "shell.execute_reply": "2023-02-28T18:59:18.562353Z"
    },
    "papermill": {
     "duration": 0.094019,
     "end_time": "2023-02-28T18:59:18.566510",
     "exception": false,
     "start_time": "2023-02-28T18:59:18.472491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "Tweet data\n",
      "----------------------------------------------------------------------------------\n",
      "there are  2600 tweets that are not sarcastic\n",
      "there are  867 tweets that are sarcastic\n",
      "there are  713 tweets with pure sarcasm\n",
      "there are  155 tweets that are ironic\n",
      "there are  25 tweets with satire\n",
      "there are  10 tweets that are understatement\n",
      "there are  40 tweets that are overstatement\n",
      "there are  101 tweets that are rhetorical question\n",
      "----------------------------------------------------------------------------------\n",
      "News Headlines data\n",
      "----------------------------------------------------------------------------------\n",
      "there are  14985 news that are not sarcastic\n",
      "there are  13634 news that are sarcastic\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------------------------------')\n",
    "print('Tweet data')\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print('there are ',data.loc[data.sarcastic == 0].shape[0],'tweets that are not sarcastic')\n",
    "print('there are ',data.loc[data.sarcastic == 1].shape[0],'tweets that are sarcastic')\n",
    "print('there are ',data.loc[data.sarcasm == 1].shape[0],'tweets with pure sarcasm')\n",
    "print('there are ',data.loc[data.irony == 1].shape[0],'tweets that are ironic')\n",
    "print('there are ',data.loc[data.satire == 1].shape[0],'tweets with satire')\n",
    "print('there are ',data.loc[data.understatement == 1].shape[0],'tweets that are understatement')\n",
    "print('there are ',data.loc[data.overstatement == 1].shape[0],'tweets that are overstatement')\n",
    "print('there are ',data.loc[data.rhetorical_question == 1].shape[0],'tweets that are rhetorical question')\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print('News Headlines data')\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print('there are ',news.loc[news.is_sarcastic == 0].shape[0],'news that are not sarcastic')\n",
    "print('there are ',news.loc[news.is_sarcastic == 1].shape[0],'news that are sarcastic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c773295",
   "metadata": {
    "papermill": {
     "duration": 0.067473,
     "end_time": "2023-02-28T18:59:18.699967",
     "exception": false,
     "start_time": "2023-02-28T18:59:18.632494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7510c777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T18:59:18.835643Z",
     "iopub.status.busy": "2023-02-28T18:59:18.834850Z",
     "iopub.status.idle": "2023-02-28T18:59:18.846453Z",
     "shell.execute_reply": "2023-02-28T18:59:18.845116Z"
    },
    "papermill": {
     "duration": 0.081173,
     "end_time": "2023-02-28T18:59:18.849453",
     "exception": false,
     "start_time": "2023-02-28T18:59:18.768280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "quotes = ['\"', '“', '”',\"‘\", \"’\", \"'\"]\n",
    "words_to_replace = {\n",
    "                    \"won't\":\"will not\",\n",
    "                    \"can't\":\"can not\",\n",
    "                    }\n",
    "\n",
    "def replace_words(x, dict):\n",
    "    for word, replacement in dict.items():\n",
    "        x = x.replace(word, replacement)\n",
    "    return x\n",
    "\n",
    "def remove_emoji(x):\n",
    "    '''\n",
    "    Function to remove emojis, symbols and pictograms etc from text\n",
    "    @param text: (str) sentences \n",
    "    @return: (str) clean text \n",
    "    '''\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', x)\n",
    "\n",
    "def pos_tagging(x):\n",
    "    x = nltk.sent_tokenize(x)\n",
    "    x = [nltk.word_tokenize(sent) for sent in x]\n",
    "    x = [nltk.pos_tag(sent) for sent in x]\n",
    "    return [num for sublist in x for num in sublist]\n",
    "\n",
    "def extract_pos(x):\n",
    "    return [sublist[1] for sublist in x]\n",
    "\n",
    "def extract_pos_pattern(x):\n",
    "    return \" \".join([label for label, group in groupby(x)])\n",
    "\n",
    "def chunking(x):\n",
    "    l = []\n",
    "    z = nltk.chunk.ne_chunk(x)\n",
    "    l.append(z)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f780c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T18:59:18.990411Z",
     "iopub.status.busy": "2023-02-28T18:59:18.989636Z",
     "iopub.status.idle": "2023-02-28T18:59:19.004134Z",
     "shell.execute_reply": "2023-02-28T18:59:19.002739Z"
    },
    "papermill": {
     "duration": 0.0899,
     "end_time": "2023-02-28T18:59:19.007097",
     "exception": false,
     "start_time": "2023-02-28T18:59:18.917197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_cleansing(df,text_field):\n",
    "    df['clean_data'] = df[text_field].str.lower()\n",
    "\n",
    "    df['tags'] = df[text_field].apply(lambda x: ','.join(re.findall(r'(@[^\\s]+)', str(x))))\n",
    "    df['hashtags'] = df[text_field].apply(lambda x: ','.join(re.findall(r'(#[^\\s]+)', str(x))))\n",
    "    df['url'] = df[text_field].apply(lambda x: ','.join(re.findall(r'(https?://[^\\s]+)', str(x))))\n",
    "\n",
    "    df['clean_data'] = df['clean_data'].apply(lambda x: re.sub(r'(@[^\\s]+)', '', str(x)))\n",
    "    df['clean_data'] = df['clean_data'].apply(lambda x: re.sub(r'(#[^\\s]+)', '', str(x)))\n",
    "    df['clean_data'] = df['clean_data'].apply(lambda x: re.sub(r'(https?://[^\\s]+)', '', str(x)))\n",
    "\n",
    "    df['clean_data'] = df['clean_data'].apply(lambda x: replace_words(str(x), words_to_replace))\n",
    "    df['clean_data'] = df['clean_data'].str.replace(\"n't\",\" not\")\n",
    "    df['clean_data'] = df['clean_data'].apply(lambda x: str(x).translate(str.maketrans('', '', string.punctuation)))\n",
    "    df['clean_data'] = df['clean_data'].str.replace('|'.join(quotes),\"\")\n",
    "    df['clean_data'] = df['clean_data'].apply(remove_emoji)\n",
    "    df['clean_data_ws'] = df['clean_data'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    df['pos_tagged_data'] = df['clean_data'].apply(lambda x: str(pos_tagging(x)).strip('[]'))\n",
    "    df['pos'] = df['clean_data'].apply(lambda x: \" \".join(map(str,extract_pos(pos_tagging(x)))))\n",
    "    df['pos_pattern'] = df['clean_data'].apply(lambda x: extract_pos_pattern(extract_pos(pos_tagging(x))))\n",
    "    df['chunked_data'] = df['clean_data'].apply(lambda x: chunking(pos_tagging(x)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7fc8f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T18:59:19.143615Z",
     "iopub.status.busy": "2023-02-28T18:59:19.142485Z",
     "iopub.status.idle": "2023-02-28T19:02:35.883949Z",
     "shell.execute_reply": "2023-02-28T19:02:35.882199Z"
    },
    "papermill": {
     "duration": 196.813652,
     "end_time": "2023-02-28T19:02:35.887810",
     "exception": false,
     "start_time": "2023-02-28T18:59:19.074158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_news = data_cleansing(news[['headline']], 'headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee146e1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T19:02:36.019102Z",
     "iopub.status.busy": "2023-02-28T19:02:36.018399Z",
     "iopub.status.idle": "2023-02-28T19:03:13.107524Z",
     "shell.execute_reply": "2023-02-28T19:03:13.106477Z"
    },
    "papermill": {
     "duration": 37.156584,
     "end_time": "2023-02-28T19:03:13.110060",
     "exception": false,
     "start_time": "2023-02-28T19:02:35.953476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_tweet = data_cleansing(data[['tweet']],'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280c9929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T19:03:13.239549Z",
     "iopub.status.busy": "2023-02-28T19:03:13.238908Z",
     "iopub.status.idle": "2023-02-28T19:03:13.281341Z",
     "shell.execute_reply": "2023-02-28T19:03:13.280152Z"
    },
    "papermill": {
     "duration": 0.110604,
     "end_time": "2023-02-28T19:03:13.284094",
     "exception": false,
     "start_time": "2023-02-28T19:03:13.173490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_df = clean_tweet[['clean_data','clean_data_ws','pos_tagged_data','pos']].join(data[['sarcastic','sarcasm','irony','satire','understatement','overstatement','rhetorical_question']])\n",
    "news_df = clean_news[['clean_data','clean_data_ws','pos_tagged_data','pos']].join(news[['is_sarcastic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2e2e90a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T19:03:13.413455Z",
     "iopub.status.busy": "2023-02-28T19:03:13.413052Z",
     "iopub.status.idle": "2023-02-28T19:03:13.739891Z",
     "shell.execute_reply": "2023-02-28T19:03:13.738480Z"
    },
    "papermill": {
     "duration": 0.395196,
     "end_time": "2023-02-28T19:03:13.742461",
     "exception": false,
     "start_time": "2023-02-28T19:03:13.347265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_df.to_csv('tweet_data.csv')\n",
    "news_df.to_csv('news_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b56a1",
   "metadata": {
    "papermill": {
     "duration": 0.063153,
     "end_time": "2023-02-28T19:03:13.869212",
     "exception": false,
     "start_time": "2023-02-28T19:03:13.806059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create vectors and embedded texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4efad1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T19:03:13.999660Z",
     "iopub.status.busy": "2023-02-28T19:03:13.998894Z",
     "iopub.status.idle": "2023-02-28T19:03:14.022709Z",
     "shell.execute_reply": "2023-02-28T19:03:14.021207Z"
    },
    "papermill": {
     "duration": 0.092056,
     "end_time": "2023-02-28T19:03:14.025705",
     "exception": false,
     "start_time": "2023-02-28T19:03:13.933649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_vectors_embeddings(df, TEXT, LABEL, prefix):\n",
    "    train_x_sw, valid_x_sw, y_train_sw, y_valid_sw = model_selection.train_test_split(df[TEXT+'_ws'], df[LABEL], random_state=42, stratify=df[LABEL], test_size=0.2)\n",
    "\n",
    "    # For Embeddings\n",
    "    train_x, valid_x, y_train, y_valid = model_selection.train_test_split(df[TEXT], df[LABEL], random_state=42, stratify=df[LABEL], test_size=0.2)\n",
    "\n",
    "    # label encode the target variable \n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    train_y_sw = encoder.fit_transform(y_train_sw)\n",
    "    valid_y_sw = encoder.fit_transform(y_valid_sw)\n",
    "    train_y = encoder.fit_transform(y_train)\n",
    "    valid_y = encoder.fit_transform(y_valid)\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train), y = y_train)\n",
    "    print(*[f'Class weight: {round(i[0],4)}\\tclass: {i[1]}' for i in zip(class_weights, np.unique(y_train))], sep='\\n')\n",
    "\n",
    "    # Determined if the dataset is balanced or imbalanced \n",
    "    ratio = np.min(df[LABEL].value_counts()) / np.max(df[LABEL].value_counts())\n",
    "    if ratio > 0.1:      # Ratio 1:10 -> limite blanced / imbalanced \n",
    "        balanced = True\n",
    "        print(f\"\\nThe dataset is balanced (ratio={round(ratio, 3)})\")\n",
    "    else:\n",
    "        balanced = False\n",
    "        print(f\"\\nThe dataset is imbalanced (ratio={round(ratio, 3)})\")\n",
    "        #from imblearn.over_sampling import ADASYN\n",
    "        # put class for debalanced data \n",
    "        # in progress\n",
    "\n",
    "    # Keep the unique label corresponding to their encoding correspondance\n",
    "    labels = df[LABEL].unique()\n",
    "    test=pd.DataFrame(data=np.transpose([labels,encoder.fit_transform(labels)]), columns=[\"labels\", \"encoding\"]).sort_values(by=[\"encoding\"])\n",
    "    labels=test.labels.tolist()\n",
    "\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    count_vect.fit(df[TEXT]+\"_sw\")\n",
    "\n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x_sw)\n",
    "    xvalid_count =  count_vect.transform(valid_x_sw)\n",
    "\n",
    "    # word level tf-idf\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "    tfidf_vect.fit(df[TEXT])\n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x_sw)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x_sw)\n",
    "    print(\"word level tf-idf done\")\n",
    "    # ngram level tf-idf \n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram.fit(df[TEXT])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x_sw)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x_sw)\n",
    "    print(\"ngram level tf-idf done\")\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) #token_pattern=r'\\w{1,}',\n",
    "    tfidf_vect_ngram_chars.fit(df[TEXT])\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x_sw) \n",
    "    xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x_sw) \n",
    "    print(\"characters level tf-idf done\")\n",
    "\n",
    "    # create a tokenizer \n",
    "    token = Tokenizer()\n",
    "    token.fit_on_texts(df[TEXT])\n",
    "    word_index = token.word_index\n",
    "\n",
    "    # convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "    train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=300)\n",
    "    valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=300)\n",
    "\n",
    "    # create token-embedding mapping\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    words = []\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)\n",
    "        words.append(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    with open(prefix+'_x_values.pkl', 'wb') as f:\n",
    "        pkl.dump(xtrain_count, f)\n",
    "        pkl.dump(xtrain_tfidf, f)\n",
    "        pkl.dump(xtrain_tfidf_ngram, f)\n",
    "        pkl.dump(xtrain_tfidf_ngram_chars, f)\n",
    "        pkl.dump(train_seq_x, f)\n",
    "        pkl.dump(xvalid_count, f)\n",
    "        pkl.dump(xvalid_tfidf, f)\n",
    "        pkl.dump(xvalid_tfidf_ngram, f)\n",
    "        pkl.dump(xvalid_tfidf_ngram_chars, f)\n",
    "        pkl.dump(valid_seq_x, f)\n",
    "\n",
    "    with open(prefix+'_y_values.pkl', 'wb') as f:\n",
    "        pkl.dump(train_y_sw, f)\n",
    "        pkl.dump(train_y, f)\n",
    "        pkl.dump(valid_y_sw, f)\n",
    "        pkl.dump(valid_y, f)\n",
    "\n",
    "    with open(prefix+'_word_index_labels_weights.pkl', 'wb') as f:\n",
    "        pkl.dump(word_index, f)\n",
    "        pkl.dump(labels, f)\n",
    "        pkl.dump(class_weights, f)\n",
    "\n",
    "    with open(prefix+'_embedding_matrix.pkl', 'wb') as f:\n",
    "        pkl.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c62803f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T19:03:14.153864Z",
     "iopub.status.busy": "2023-02-28T19:03:14.153452Z",
     "iopub.status.idle": "2023-02-28T19:03:16.192638Z",
     "shell.execute_reply": "2023-02-28T19:03:16.191289Z"
    },
    "papermill": {
     "duration": 2.106098,
     "end_time": "2023-02-28T19:03:16.195325",
     "exception": false,
     "start_time": "2023-02-28T19:03:14.089227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: 0.6666\tclass: 0\n",
      "Class weight: 2.0007\tclass: 1\n",
      "\n",
      "The dataset is balanced (ratio=0.333)\n",
      "word level tf-idf done\n",
      "ngram level tf-idf done\n",
      "characters level tf-idf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9567/9567 [00:00<00:00, 59793.90it/s]\n"
     ]
    }
   ],
   "source": [
    "create_vectors_embeddings(tweet_df, 'clean_data', 'sarcastic', 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c2ee67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T19:03:16.412579Z",
     "iopub.status.busy": "2023-02-28T19:03:16.412164Z",
     "iopub.status.idle": "2023-02-28T19:03:27.813669Z",
     "shell.execute_reply": "2023-02-28T19:03:27.812415Z"
    },
    "papermill": {
     "duration": 11.469997,
     "end_time": "2023-02-28T19:03:27.816543",
     "exception": false,
     "start_time": "2023-02-28T19:03:16.346546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: 0.9549\tclass: 0\n",
      "Class weight: 1.0496\tclass: 1\n",
      "\n",
      "The dataset is balanced (ratio=0.91)\n",
      "word level tf-idf done\n",
      "ngram level tf-idf done\n",
      "characters level tf-idf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29599/29599 [00:00<00:00, 56802.58it/s]\n"
     ]
    }
   ],
   "source": [
    "create_vectors_embeddings(news_df, 'clean_data', 'is_sarcastic', 'news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de389005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T19:03:27.949187Z",
     "iopub.status.busy": "2023-02-28T19:03:27.948738Z",
     "iopub.status.idle": "2023-02-28T19:03:29.025105Z",
     "shell.execute_reply": "2023-02-28T19:03:29.024087Z"
    },
    "papermill": {
     "duration": 1.14651,
     "end_time": "2023-02-28T19:03:29.027629",
     "exception": false,
     "start_time": "2023-02-28T19:03:27.881119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.remove(\"/kaggle/working/crawl-300d-2M-subword.bin\")\n",
    "os.remove(\"/kaggle/working/crawl-300d-2M-subword.vec\")\n",
    "os.remove(\"/kaggle/working/crawl-300d-2M-subword.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 657.926678,
   "end_time": "2023-02-28T19:03:32.098244",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-28T18:52:34.171566",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
